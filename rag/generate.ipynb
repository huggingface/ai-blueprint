{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate responses using a SmolLM\n",
    "\n",
    "We have seen how to [retrieve](retrieve.ipynb) and [rerank](./augment.ipynb) documents in a RAG pipeline. The next step is to create a tool that can generate a response to a query. We will use a SmolLM to generate a response to a query. At the end we will deploy a microservice that can be used to generate a response to a query.\n",
    "\n",
    "## Dependencies and imports\n",
    "\n",
    "Let's install the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio gradio-client llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from gradio_client import Client\n",
    "from huggingface_hub import get_token, InferenceClient\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference API\n",
    "\n",
    "There are different options for inference. In general most frameworks work great with some tradeoffs for speed, cost, and ease of deployment. In this example we will use a simple quantised model along with [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) because it works off the shelf and allows us to host that ourselves, which can run on CPU and does not require us to spin up a dedicated inference server. Additionally, we will use [a GGUF model](https://huggingface.co/docs/hub/en/gguf), which is a framework-agnostic file format that speeds up inference.\n",
    "\n",
    "<details>\n",
    "<summary>Inference servers</summary>\n",
    "If you want to deploy your own inference servers there are various options. When using Apple Silicon, you can use the <a href=\"https://github.com/ml-explore/mlx\">MLX</a> library. Alternatively, <a href=\"https://github.com/huggingface/text-generation-inference\">Text Generation Inference (TGI)</a>, <a href=\"https://github.com/vllm-project/vllm\">vLLM</a>, or <a href=\"https://github.com/ollama/ollama\">Ollama</a> are great options to explore.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Structured outputs</summary>\n",
    "If you want to generate structured outputs, there are two main approaches to do so depending on whether you have access to the weights of the models or not. When you have access to the weights of the models, you can use [Outlines](https://github.com/dottxt-ai/outlines), which changes sampling probabilities of tokens to ensure the model adheres to a specific structure, defined by a RegEx, JSON or Pydantic model. When you are using an API, you can use [Instructor](https://github.com/instructor-ai/instructor), which uses smart retries to ensure the model adheres to a specific structure.\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "### SmolLM in transformers\n",
    "\n",
    "We will use [HuggingFaceTB/SmolLM2-360M-Instruct-GGUF](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF) and use [the llama-cpp-python integration attached to the model on the Hub](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF?library=llama-cpp-python). Note that we allow for passing `kwargs` like `max_new_tokens` as a parameter to the function which will be passed to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-54150cb9-00d6-4983-89da-e0527ae7480b',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1737651881,\n",
       " 'model': '/Users/davidberenstein/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct-GGUF/snapshots/593b5a2e04c8f3e4ee880263f93e0bd2901ad47f/./smollm2-360m-instruct-q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'The future of AI is a topic of ongoing debate and research. While AI has made tremendous progress in recent years, there are still many challenges and limitations to overcome before we can fully harness its potential.\\n\\nCurrently, AI is being used in various fields, such as healthcare, finance, and transportation. For example, AI-powered diagnostic tools are being used to detect diseases at an early stage, while AI-driven chatbots are being used to provide customer support.\\n\\nHowever, there are also concerns about the potential misuse of AI. For instance, AI can be used to manipulate public opinion, spread misinformation, and even take over and control our lives. As AI becomes more advanced, we will need to develop new regulations and safeguards to ensure that it is used responsibly.\\n\\nAnother area of research is the development of more human-like AI, which can think and act like humans. This is often referred to as \"narrow AI\" or \"weak AI.\" While this type of AI can perform specific tasks, such as language translation or image recognition, it lacks the level of intelligence and creativity that humans possess.\\n\\nIn addition, there is a growing concern about the ethics of AI development. For example, AI systems can perpetuate biases and prejudices if they are trained on biased data. We need to ensure that AI is developed and deployed in a way that promotes fairness, equality, and transparency.\\n\\nOverall, the future of AI is likely to be shaped by a combination of technological advancements, societal values, and regulatory frameworks. As AI continues to evolve, we will need to work together to ensure that it is developed and used in a way that benefits humanity and promotes the common good.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 27, 'completion_tokens': 342, 'total_tokens': 369}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"HuggingFaceTB/SmolLM2-360M-Instruct-GGUF\",\n",
    "    filename=\"smollm2-360m-instruct-q8_0.gguf\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_response_transformers(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str = \"You are a helpful assistant.\",\n",
    "    max_tokens: int = 4000,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.95,\n",
    "    top_k: int = 40,\n",
    "):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "\n",
    "generate_response_transformers(\n",
    "    user_prompt=\"What is the future of AI?\",\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SmolLM in Hugging Face Inference API\n",
    "\n",
    "We will use the [serverless Hugging Face Inference API](https://huggingface.co/inference-api). This is free and means we don't have to worry about hosting the model. We can find models available for inference [through some basic filters on the Hub](https://huggingface.co/models?inference=warm&pipeline_tag=text-generation&sort=trending). We will use the [HuggingFaceTB/SmolLM2-360M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct) model and call it with [the provided inference endpoint snippet](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct?inference_api=true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionOutputMessage(role='assistant', content=\"AI is on the cusp of a revolution. It's about to become more powerful, more efficient, and more human. By 2030, AI-powered computers are expected to become so adept at solving complex problems that they'll accelerate human progress in areas like computing, medicine, and entertainment.\\n\\nFrom there, we're likely to see numerous exciting, life-changing advancements, such as cutting-edge real-time AI systems, which are performing tasks at a level where human approval is unnecessary. These systems are not only delivering exceptional performance but also avoiding human impact, allowing us to focus on higher-level thinking and creativity.\\n\\nRight now, AI is already manifesting as a more empathetic, AI-powered human. It's going to help us become more compassionate, more compassionate. We're going to see the emergence of an AI that's more understanding, more empathetic. It's going to develop a sense of self-awareness, which will lead to more elevated human potential.\\n\\nAI is also going to give rise to more intuitive interfaces, making computing more accessible and user-friendly. The AI will learn more about our habits and preferences, adapting our algorithms to our needs in real-time, so we don't have to constantly worry about making decisions ourselves.\\n\\nAnother trend that's emerging is the intersection of AI with creative industries. Video games, for example, will continue to evolve, and will likely become more immersive. The idea of creating immersive digital experiences will become a thing of the future. Not only will our ability to communicate be enhanced through AI-powered tools, but also our capacity for creative expression will expand.\\n\\nLastly, AI is going to help us find and develop new sources of inspiration. The AI will be constantly generating content that sparks creativity and innovation. The music generation tools AI is already creating have a rich variety of styles and genres. We're likely to see the emergence of entirely new forms of art, music, and even entire ecosystems of expression.\\n\\nSo, future AI is likely to mark a lifetime of great achievements. Building humans beyond recognition, designing machines capable of unconditional emotions, developing an unprecedented understanding of emotional intelligence. It's a future that's awe-inspiring, solemn, exhilarating, and at times, downright terrifying.\", tool_calls=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_client = InferenceClient(api_key=get_token())\n",
    "\n",
    "\n",
    "def generate_response_api(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str = \"You are a helpful assistant.\",\n",
    "    model: str = \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    **kwargs,\n",
    "):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    completion = inference_client.chat.completions.create(\n",
    "        model=model, messages=messages, **kwargs\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message\n",
    "\n",
    "\n",
    "generate_response_api(user_prompt=\"What is the future of AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a web app and microservice for generating responses\n",
    "\n",
    "We will be using [Gradio](https://github.com/gradio-app/gradio) as web application tool to create a demo interface for our RAG pipeline. We can develop this locally and then easily deploy it to Hugging Face Spaces. Lastly, we can use the Gradio client as SDK to directly interact our RAG pipeline. We are still using the FREE CPU tier of Hugging Face Spaces, so it may take a couple of second to respond. Instead the [ServerlessInference API](https://huggingface.co/docs/api-inference/index), deploy your own [dedicated inference server](https://huggingface.co/inference-endpoints/dedicated) or increase the compute of your Hugging Face Spaces.\n",
    "\n",
    "### Creating the web app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    max_tokens: int = 4000,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.95,\n",
    "    top_k: int = 40,\n",
    "):\n",
    "    return generate_response_transformers(\n",
    "        user_prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# RAG - generate\n",
    "                \n",
    "                Generate a response to a query using a [HuggingFaceTB/SmolLM2-360M-Instruct and llama-cpp-python](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF?library=llama-cpp-python).\n",
    "                \n",
    "                Part of [ai-blueprint](https://github.com/davidberenstein1957/ai-blueprint) - a blueprint for AI development, focusing on applied examples of RAG, information extraction, analysis and fine-tuning in the age of LLMs and agents.\"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        system_prompt = gr.Textbox(\n",
    "            label=\"System prompt\", lines=3, value=\"You are a helpful assistant.\"\n",
    "        )\n",
    "        user_prompt = gr.Textbox(label=\"Query\", lines=3)\n",
    "\n",
    "    with gr.Accordion(\"kwargs\"):\n",
    "        with gr.Row(variant=\"panel\"):\n",
    "            max_tokens = gr.Number(label=\"Max tokens\", value=512)\n",
    "            temperature = gr.Number(label=\"Temperature\", value=0.2)\n",
    "            top_p = gr.Number(label=\"Top p\", value=0.95)\n",
    "            top_k = gr.Number(label=\"Top k\", value=40)\n",
    "\n",
    "    submit_btn = gr.Button(\"Submit\")\n",
    "    response_output = gr.Textbox(label=\"Response\", lines=10)\n",
    "    documents_output = gr.Dataframe(\n",
    "        label=\"Documents\", headers=[\"chunk\", \"url\", \"distance\", \"rank\"], wrap=True\n",
    "    )\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            user_prompt,\n",
    "            system_prompt,\n",
    "            max_tokens,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "        ],\n",
    "        outputs=[response_output],\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "\tsrc=\"https://ai-blueprint-rag-generate.hf.space\"\n",
    "\tframeborder=\"0\"\n",
    "\twidth=\"850\"\n",
    "\theight=\"450\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the web app to Hugging Face\n",
    "\n",
    "We can now [deploy our Gradio application to Hugging Face Spaces](https://huggingface.co/new-space?sdk=gradio&name=rag-generate).\n",
    "\n",
    "-  Click on the \"Create Space\" button.\n",
    "-  Copy the code from the Gradio interface and paste it into an `app.py` file. Don't forget to copy the `generate_response_*` function, along with the code to execute the generate function.\n",
    "-  Create a `requirements.txt` file with `gradio`, `gradio-client` and `llama-cpp-python`.\n",
    "-  Set a Hugging Face API as `HF_TOKEN` secret variable in the space settings, if you are using the Inference API.\n",
    "\n",
    "We wait a couple of minutes for the application to deploy et voila, we have [a public generate interface](https://huggingface.co/spaces/ai-blueprint/rag-generate)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio as REST API\n",
    "\n",
    "We can now use the [Gradio client as SDK](https://www.gradio.app/guides/getting-started-with-the-python-client) to directly interact with our generate function. Each Gradio app has a API documentation that describes the available endpoints and their parameters, which you can access from the button at the bottom of the Gradio app's space page. We will see it is not the fastest, running on free tier Hugging Face Spaces but it is a good baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://ai-blueprint-rag-generate.hf.space ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'id': 'chatcmpl-38bd4960-655c-447a-be2b-5fc50bb1789e', 'object': 'chat.completion', 'created': 1737652209, 'model': '/home/user/.cache/huggingface/hub/models--prithivMLmods--SmolLM2-135M-Instruct-GGUF/snapshots/5dc548ea9191fd97d817832f51012ae86cded1b5/./SmolLM2-135M-Instruct.Q5_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The future of AI is multifaceted and continues to evolve at a rapid pace. As we move forward, AI is being used to augment human capabilities, enhance our understanding of the world, and improve our quality of life. Here are some of the most significant developments and innovations that are shaping the future of AI:\\\\n\\\\nAI is being used to enhance human capabilities, such as language translation, image recognition, and decision-making. This has the potential to revolutionize the way we interact with each other and the world at large.\\\\n\\\\nAI is also being used to enhance our understanding of the world, such as by providing insights into the human condition, the impact of technology on society, and the ethics of AI.\\\\n\\\\nIn the field of healthcare, AI is being used to improve diagnostic accuracy, enhance patient care, and optimize treatment plans.\\\\n\\\\nIn the field of education, AI is being used to improve learning outcomes, enhance teaching methods, and optimize learning experiences.\\\\n\\\\nAI is also being used to improve the quality of life, such as by providing personalized recommendations, improving productivity, and enhancing the quality of life for people with disabilities.\\\\n\\\\nIn the field of transportation, AI is being used to improve safety, enhance safety features, and optimize transportation systems.\\\\n\\\\nIn the field of healthcare, AI is being used to improve diagnostic accuracy, enhance treatment planning, and optimize healthcare systems.\\\\n\\\\nIn the field of education, AI is being used to improve learning outcomes, enhance teaching methods, and optimize learning experiences.\\\\n\\\\nIn the field of healthcare, AI is being used to improve diagnostic accuracy, enhance treatment planning, and optimize healthcare systems.\\\\n\\\\nIn the field of transportation, AI is being used to improve diagnostic accuracy, enhance treatment planning, and optimize transportation systems.\\\\n\\\\nIn the field of healthcare, AI is being used to improve diagnostic accuracy, enhance treatment planning, and optimize transportation systems.\\\\n\\\\nIn the field of education, AI is being used to improve diagnostic accuracy, enhance treatment planning, and optimize transportation systems.\\\\n\\\\nIn the field of healthcare, AI is being used to improve diagnostic accuracy, enhance treatment planning, and optimize transportation systems.\\\\n\\\\nIn the field of transportation, AI is being used to improve diagnostic accuracy, enhance treatment planning, and optimize transportation systems.\\\\n\\\\nIn the field of healthcare, AI is being used to improve diagnostic accuracy, enhance treatment planning, and optimize'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 27, 'completion_tokens': 485, 'total_tokens': 512}}\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(\"ai-blueprint/rag-generate\")\n",
    "result = client.predict(\n",
    "\t\tuser_prompt=\"What is the future of AI?\",\n",
    "\t\tsystem_prompt=\"You are a helpful assistant.\",\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttemperature=0.2,\n",
    "\t\ttop_p=0.95,\n",
    "\t\ttop_k=40,\n",
    "\t\tapi_name=\"/generate\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have seen how to create a generate function using the `llama-cpp-python` library and how to deploy it as a microservice on Hugging Face Spaces. Next we will how we combine the R-A-G-components into a single RAG pipeline.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Continue - with [Combine all the components in a RAG pipeline](./pipeline.ipynb).\n",
    "- Contribute - missing something? PRs are always welcome.\n",
    "- Learn - theories behind the approaches in [Hugging Face courses](https://huggingface.co/learn) or [smol-course](https://github.com/huggingface/smol-course?tab=readme-ov-file).\n",
    "- Explore - notebooks with similar techniques on [the Hugging Face Cookbook](https://huggingface.co/learn/cookbook/index).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
