{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment retrieval results by reranking using Sentence Transformers\n",
    "\n",
    "Retrieval are quick estimates of the most relevant documents to a query which works fine for a first pass over millions of documents, but we can improve this relevance by reranking the retrieved documents. We will be using the retrieval microservice of the [Index and retrieve documents for vector search using Sentence Transformers and DuckDB](./retrieve.ipynb) notebook to build a reranker. We have seen how to index the Hugging Face Hub and perform vector search on it. Now, we will build a reranker that takes a set of documents and a query and returns a list of documents sorted by relevance to the query.\n",
    "\n",
    "## Hugging Face as a vector search backend\n",
    "\n",
    "A brief recap of the previous notebook, we use Hugging Face as vector search backend and can call it as a REST API through the Gradio Python Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://smol-blueprint-rag-retrieval.hf.space/ ✔\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>url</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The last decade was a big one for artificial i...</td>\n",
       "      <td>https://www.bbc.com/news/technology-51064369</td>\n",
       "      <td>0.260197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"The manifold of things which were lumped into...</td>\n",
       "      <td>https://www.bbc.com/news/technology-51064369</td>\n",
       "      <td>0.358736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Artificial intelligence (AI) is one of the mos...</td>\n",
       "      <td>https://www.bbc.co.uk/news/business-48139212</td>\n",
       "      <td>0.364774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nSeveral others started to talk about AGI be...</td>\n",
       "      <td>https://www.bbc.com/news/technology-51064369</td>\n",
       "      <td>0.370624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can artificial intelligence predict the future...</td>\n",
       "      <td>https://www.bbc.com/news/av/technology-46104433</td>\n",
       "      <td>0.376021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  The last decade was a big one for artificial i...   \n",
       "1  \"The manifold of things which were lumped into...   \n",
       "2  Artificial intelligence (AI) is one of the mos...   \n",
       "3  \"\\nSeveral others started to talk about AGI be...   \n",
       "4  Can artificial intelligence predict the future...   \n",
       "\n",
       "                                               url  distance  \n",
       "0     https://www.bbc.com/news/technology-51064369  0.260197  \n",
       "1     https://www.bbc.com/news/technology-51064369  0.358736  \n",
       "2     https://www.bbc.co.uk/news/business-48139212  0.364774  \n",
       "3     https://www.bbc.com/news/technology-51064369  0.370624  \n",
       "4  https://www.bbc.com/news/av/technology-46104433  0.376021  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "import pandas as pd\n",
    "\n",
    "gradio_client = Client(\"https://smol-blueprint-rag-retrieval.hf.space/\")\n",
    "\n",
    "\n",
    "def similarity_search(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    results = gradio_client.predict(api_name=\"/similarity_search\", query=query, k=k)\n",
    "    return pd.DataFrame(data=results[\"data\"], columns=results[\"headers\"])\n",
    "\n",
    "\n",
    "similarity_search(\"What is the future of AI?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking retrieved documents\n",
    "\n",
    "Whenever we retrieve documents from the vector search backend, we can improve the quality of the documents that we pass to the LLM. We do that by ranking the documents by relevance to the query. We will use the [sentence-transformers library](https://huggingface.co/sentence-transformers). You can find the best models to do this, using the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard). \n",
    "\n",
    "We will first retrieve 50 documents and then use [sentence-transformers/all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) to rerank the documents and return the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>url</th>\n",
       "      <th>distance</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Google, Facebook, Amazon join forces on future...</td>\n",
       "      <td>https://www.bbc.co.uk/news/technology-37494863</td>\n",
       "      <td>0.494021</td>\n",
       "      <td>0.503516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Google has its own AI labs and has been invest...</td>\n",
       "      <td>http://www.bbc.com/news/technology-30432493</td>\n",
       "      <td>0.391550</td>\n",
       "      <td>0.502463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>IBM says it can \"analyze and interpret all of ...</td>\n",
       "      <td>http://www.bbc.com/news/world-asia-38521403</td>\n",
       "      <td>0.443237</td>\n",
       "      <td>0.502219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Singularity: The robots are coming to steal ou...</td>\n",
       "      <td>http://www.bbc.com/news/technology-25000756</td>\n",
       "      <td>0.395984</td>\n",
       "      <td>0.501567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Google developing kill switch for AI\\n- 8 June...</td>\n",
       "      <td>http://www.bbc.com/news/technology-36472140</td>\n",
       "      <td>0.397516</td>\n",
       "      <td>0.501371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                chunk  \\\n",
       "47  Google, Facebook, Amazon join forces on future...   \n",
       "6   Google has its own AI labs and has been invest...   \n",
       "29  IBM says it can \"analyze and interpret all of ...   \n",
       "8   Singularity: The robots are coming to steal ou...   \n",
       "11  Google developing kill switch for AI\\n- 8 June...   \n",
       "\n",
       "                                               url  distance      rank  \n",
       "47  https://www.bbc.co.uk/news/technology-37494863  0.494021  0.503516  \n",
       "6      http://www.bbc.com/news/technology-30432493  0.391550  0.502463  \n",
       "29     http://www.bbc.com/news/world-asia-38521403  0.443237  0.502219  \n",
       "8      http://www.bbc.com/news/technology-25000756  0.395984  0.501567  \n",
       "11     http://www.bbc.com/news/technology-36472140  0.397516  0.501371  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import pandas as pd\n",
    "\n",
    "reranker = CrossEncoder(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "\n",
    "def rerank_documents(query: str, documents: pd.DataFrame) -> pd.DataFrame:\n",
    "    documents = documents.copy()\n",
    "    documents = documents.drop_duplicates(\"chunk\")\n",
    "    documents[\"rank\"] = reranker.predict([[query, hit] for hit in documents[\"chunk\"]])\n",
    "    documents = documents.sort_values(by=\"rank\", ascending=False)\n",
    "    return documents\n",
    "\n",
    "\n",
    "query = \"What is the future of AI?\"\n",
    "documents = similarity_search(query, k=50)\n",
    "reranked_documents = rerank_documents(query=query, documents=documents)\n",
    "reranked_documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the returned documents have slightly shifted in the ranking, which is good, because we see that our reranking works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a web app and microservice for reranking\n",
    "\n",
    "We will be using [Gradio](https://github.com/gradio-app/gradio) as web application tool to create a demo interface for our RAG pipeline. We can develop this locally and then easily deploy it to Hugging Face Spaces. Lastly, we can use the Gradio client as SDK to directly interact our RAG pipeline.\n",
    "\n",
    "### Creating the web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def rag_interface(query: str, documents: pd.DataFrame):\n",
    "    documents = rerank_documents(query=query, documents=documents)\n",
    "    return documents\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# RAG Hub Datasets \n",
    "                \n",
    "                Part of [smol blueprint](https://github.com/davidberenstein1957/smol-blueprint) - a smol blueprint for AI development, focusing on practical examples of RAG, information extraction, analysis and fine-tuning in the age of LLMs.\"\"\")\n",
    "\n",
    "    query_input = gr.Textbox(\n",
    "        label=\"Query\", placeholder=\"Enter your question here...\", lines=3\n",
    "    )\n",
    "    documents_input = gr.Dataframe(\n",
    "        label=\"Documents\", headers=[\"chunk\"], wrap=True, interactive=True\n",
    "    )\n",
    "\n",
    "    submit_btn = gr.Button(\"Submit\")\n",
    "    documents_output = gr.Dataframe(\n",
    "        label=\"Documents\", headers=[\"chunk\", \"rank\"], wrap=True\n",
    "    )\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn=rerank_documents,\n",
    "        inputs=[query_input, documents_input],\n",
    "        outputs=[documents_output],\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "\tsrc=\"https://smol-blueprint-rag-hub-datasets.hf.space\"\n",
    "\tframeborder=\"0\"\n",
    "\twidth=\"850\"\n",
    "\theight=\"450\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the web app to Hugging Face\n",
    "\n",
    "We can now [deploy our Gradio application to Hugging Face Spaces](https://huggingface.co/new-space?sdk=gradio&name=rag-augment).\n",
    "\n",
    "-  Click on the \"Create Space\" button.\n",
    "-  Copy the code from the Gradio interface and paste it into an `app.py` file. Don't forget to copy the `generate_response_*` function, along with the code to execute the RAG pipeline.\n",
    "-  Create a `requirements.txt` file with `gradio-client` and `sentence-transformers`.\n",
    "-  Set a Hugging Face API as `HF_TOKEN` secret variable in the space settings, if you are using the Inference API.\n",
    "\n",
    "We wait a couple of minutes for the application to deploy et voila, we have [a public RAG interface](https://huggingface.co/spaces/smol-blueprint/rag-augment)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the web app as a microservice\n",
    "\n",
    "We can now use the [Gradio client as SDK](https://www.gradio.app/guides/getting-started-with-the-python-client) to directly interact with our RAG pipeline. Each Gradio app has a API documentation that describes the available endpoints and their parameters, which you can access from the button at the bottom of the Gradio app's space page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://smol-blueprint-rag-augment.hf.space/ ✔\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not fetch config for https://smol-blueprint-rag-augment.hf.space/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m----> 3\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://smol-blueprint-rag-augment.hf.space/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m      5\u001b[0m     query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the future of AI?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     documents\u001b[38;5;241m=\u001b[39msimilarity_search(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the future of AI?\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m),\n\u001b[1;32m      7\u001b[0m     api_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/rag_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m result\n",
      "File \u001b[0;32m~/Documents/programming/smol-project/.venv/lib/python3.12/site-packages/gradio_client/client.py:153\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, src, hf_token, max_workers, verbose, auth, httpx_kwargs, headers, download_files, ssl_verify, _skip_components)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_login(auth)\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mws\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse_v2.1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mws\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m api_prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/programming/smol-project/.venv/lib/python3.12/site-packages/gradio_client/client.py:886\u001b[0m, in \u001b[0;36mClient._get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    878\u001b[0m r \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc,\n\u001b[1;32m    880\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttpx_kwargs,\n\u001b[1;32m    884\u001b[0m )\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m r\u001b[38;5;241m.\u001b[39mis_success:\n\u001b[0;32m--> 886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not fetch config for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# some basic regex to extract the config\u001b[39;00m\n\u001b[1;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.gradio_config = (.*?);[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*</script>\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not fetch config for https://smol-blueprint-rag-augment.hf.space/"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"https://smol-blueprint-rag-augment.hf.space/\")\n",
    "result = client.predict(\n",
    "    query=\"What is the future of AI?\",\n",
    "    documents=similarity_search(\"What is the future of AI?\", k=50),\n",
    "    api_name=\"/rag_pipeline\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "We have seen how to build a RAG pipeline with a SmolLM and some rerankers. Next steps would be to monitor the performance of the RAG pipeline and improve it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
