{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Hub as a vector search backend\n",
    "\n",
    "We will be using the [smol-blueprint/hf-blogs](https://huggingface.co/datasets/smol-blueprint/hf-blogs) dataset, which is a dataset that contains the blogs from the Hugging Face website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 312/312 [00:00<00:00, 9165.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'How to train a new language model from scratch using Transformers and Tokenizers',\n",
       " 'author': 'julien-c',\n",
       " 'date': 'February 14, 2020',\n",
       " 'local': 'how-to-train',\n",
       " 'tags': 'guide, nlp',\n",
       " 'URL': 'https://huggingface.co/blog/how-to-train',\n",
       " 'content': ' # How to train a new language model from scratch using Transformers and Tokenizers   <a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb\">     <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"> </a>  Over the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.  In this post we‚Äôll demo how to train a ‚Äúsmall‚Äù model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) ‚Äì that‚Äôs the same number of layers & heads as DistilBERT ‚Äì on **Esperanto**. We‚Äôll then fine-tune the model on a downstream task of part-of-speech tagging.  Esperanto is a *constructed language* with a goal of being easy to learn. We pick it for this demo for several reasons: - it is a relatively low-resource language (even though it‚Äôs spoken by ~2 million people) so this demo is less boring than training one more English model üòÅ - its grammar is highly regular (e.g. all common nouns end in -o, all adjectives in -a) so we should get interesting linguistic results even on a small dataset. - finally, the overarching goal at the foundation of the language is to bring people closer (fostering world peace and international understanding) which one could argue is aligned with the goal of the NLP community üíö  > N.B. You won‚Äôt need to understand Esperanto to understand this post, but if you do want to learn it, [Duolingo](https://www.duolingo.com/enroll/eo/en/Learn-Esperanto) has a nice course with 280k active learners.  Our model is going to be called‚Ä¶ wait for it‚Ä¶ **EsperBERTo** üòÇ  <img src=\"/blog/assets/01_how-to-train/eo.svg\" alt=\"Esperanto flag\" style=\"margin: auto; display: block; width: 260px;\">  ## 1. Find a dataset  First, let us find a corpus of text in Esperanto. Here we‚Äôll use the Esperanto portion of the [OSCAR corpus](https://traces1.inria.fr/oscar/) from INRIA. OSCAR is a huge multilingual corpus obtained by language classification and filtering of [Common Crawl](https://commoncrawl.org/) dumps of the Web.  <img src=\"/blog/assets/01_how-to-train/oscar.png\" style=\"margin: auto; display: block; width: 260px;\">  The Esperanto portion of the dataset is only 299M, so we‚Äôll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.  The final training corpus has a size of 3 GB, which is still small ‚Äì for your model, you will get better results the more data you can get to pretrain on.    ## 2. Train a tokenizer  We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let‚Äôs arbitrarily pick its size to be 52,000.  We recommend training a byte-level BPE (rather than let‚Äôs say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).  ```python #! pip install tokenizers  from pathlib import Path  from tokenizers import ByteLevelBPETokenizer  paths = [str(x) for x in Path(\"./eo_data/\").glob(\"**/*.txt\")]  # Initialize a tokenizer tokenizer = ByteLevelBPETokenizer()  # Customize training tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[     \"<s>\",     \"<pad>\",     \"</s>\",     \"<unk>\",     \"<mask>\", ])  # Save files to disk tokenizer.save_model(\".\", \"esperberto\") ```  And here‚Äôs a slightly accelerated capture of the output:  ![tokenizers](assets/01_how-to-train/tokenizers-fast.gif) <small>On our dataset, training took about ~5 minutes.</small>  üî•üî• Wow, that was fast! ‚ö°Ô∏èüî•  We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.  ```json { \\t\"<s>\": 0, \\t\"<pad>\": 1, \\t\"</s>\": 2, \\t\"<unk>\": 3, \\t\"<mask>\": 4, \\t\"!\": 5, \\t\"\\\\\"\": 6, \\t\"#\": 7, \\t\"$\": 8, \\t\"%\": 9, \\t\"&\": 10, \\t\"\\'\": 11, \\t\"(\": 12, \\t\")\": 13, \\t# ... }  # merges.txt l a ƒ† k o n ƒ† la t a ƒ† e ƒ† d ƒ† p # ... ```  What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto ‚Äì `ƒâ`, `ƒù`, `ƒ•`, `ƒµ`, `≈ù`, and `≈≠` ‚Äì are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.  Here‚Äôs how you can use it in `tokenizers`, including handling the RoBERTa special tokens ‚Äì of course, you‚Äôll also be able to use it directly from `transformers`.  ```python from tokenizers.implementations import ByteLevelBPETokenizer from tokenizers.processors import BertProcessing   tokenizer = ByteLevelBPETokenizer(     \"./models/EsperBERTo-small/vocab.json\",     \"./models/EsperBERTo-small/merges.txt\", ) tokenizer._tokenizer.post_processor = BertProcessing(     (\"</s>\", tokenizer.token_to_id(\"</s>\")),     (\"<s>\", tokenizer.token_to_id(\"<s>\")), ) tokenizer.enable_truncation(max_length=512)  print(     tokenizer.encode(\"Mi estas Julien.\") ) # Encoding(num_tokens=7, ...) # tokens: [\\'<s>\\', \\'Mi\\', \\'ƒ†estas\\', \\'ƒ†Juli\\', \\'en\\', \\'.\\', \\'</s>\\'] ```  ## 3. Train a language model from scratch  **Update:** The associated Colab notebook uses our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly, instead of through a script. Feel free to pick the approach you like best.  We will now train our language model using the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/main/examples/legacy/run_language_modeling.py) script from `transformers` (newly renamed from `run_lm_finetuning.py` as it now supports training from scratch more seamlessly). Just remember to leave `--model_name_or_path` to `None` to train from scratch vs. from an existing model or checkpoint.  > We‚Äôll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).  As the model is BERT-like, we‚Äôll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.  We just need to do two things: - implement a simple subclass of `Dataset` that loads data from our text files \\t- Depending on your use case, you might not even need to write your own subclass of Dataset, if one of the provided examples (`TextDataset` and `LineByLineTextDataset`) works ‚Äì but there are lots of custom tweaks that you might want to add based on what your corpus looks like. - Choose and experiment with different sets of hyperparameters.   Here‚Äôs a simple version of our EsperantoDataset.  ```python from torch.utils.data import Dataset  class EsperantoDataset(Dataset):     def __init__(self, evaluate: bool = False):         tokenizer = ByteLevelBPETokenizer(             \"./models/EsperBERTo-small/vocab.json\",             \"./models/EsperBERTo-small/merges.txt\",         )         tokenizer._tokenizer.post_processor = BertProcessing(             (\"</s>\", tokenizer.token_to_id(\"</s>\")),             (\"<s>\", tokenizer.token_to_id(\"<s>\")),         )         tokenizer.enable_truncation(max_length=512)         # or use the RobertaTokenizer from `transformers` directly.          self.examples = []          src_files = Path(\"./data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./data/\").glob(\"*-train.txt\")         for src_file in src_files:             print(\"üî•\", src_file)             lines = src_file.read_text(encoding=\"utf-8\").splitlines()             self.examples += [x.ids for x in tokenizer.encode_batch(lines)]      def __len__(self):         return len(self.examples)      def __getitem__(self, i):         # We‚Äôll pad at the batch level.         return torch.tensor(self.examples[i]) ```  If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step.  Here is one specific set of **hyper-parameters and arguments** we pass to the script:  ``` \\t--output_dir ./models/EsperBERTo-small-v1 \\t--model_type roberta \\t--mlm \\t--config_name ./models/EsperBERTo-small \\t--tokenizer_name ./models/EsperBERTo-small \\t--do_train \\t--do_eval \\t--learning_rate 1e-4 \\t--num_train_epochs 5 \\t--save_total_limit 2 \\t--save_steps 2000 \\t--per_gpu_train_batch_size 16 \\t--evaluate_during_training \\t--seed 42 ```  As usual, pick the largest batch size you can fit on your GPU(s).   **üî•üî•üî• Let‚Äôs start training!! üî•üî•üî•**  Here you can check our Tensorboard for [one particular set of hyper-parameters](https://tensorboard.dev/experiment/8AjtzdgPR1qG6bDIe1eKfw/#scalars):  [![tb](assets/01_how-to-train/tensorboard.png)](https://tensorboard.dev/experiment/8AjtzdgPR1qG6bDIe1eKfw/#scalars)  > Our example scripts log into the Tensorboard format by default, under `runs/`. Then to view your board just run `tensorboard dev upload --logdir runs` ‚Äì this will set up [tensorboard.dev](https://tensorboard.dev/), a Google-managed hosted version that lets you share your ML experiment with anyone.  ## 4. Check that the LM actually trained  Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.  Pipelines are simple wrappers around tokenizers and models, and the \\'fill-mask\\' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.  ```python from transformers import pipeline  fill_mask = pipeline(     \"fill-mask\",     model=\"./models/EsperBERTo-small\",     tokenizer=\"./models/EsperBERTo-small\" )  # The sun <mask>. # =>  result = fill_mask(\"La suno <mask>.\")  # {\\'score\\': 0.2526160776615143, \\'sequence\\': \\'<s> La suno brilis.</s>\\', \\'token\\': 10820} # {\\'score\\': 0.0999930202960968, \\'sequence\\': \\'<s> La suno lumis.</s>\\', \\'token\\': 23833} # {\\'score\\': 0.04382849484682083, \\'sequence\\': \\'<s> La suno brilas.</s>\\', \\'token\\': 15006} # {\\'score\\': 0.026011141017079353, \\'sequence\\': \\'<s> La suno falas.</s>\\', \\'token\\': 7392} # {\\'score\\': 0.016859788447618484, \\'sequence\\': \\'<s> La suno pasis.</s>\\', \\'token\\': 4552} ```  Ok, simple syntax/grammar works. Let‚Äôs try a slightly more interesting prompt:  ```python fill_mask(\"Jen la komenco de bela <mask>.\")  # This is the beginning of a beautiful <mask>. # =>  # { #     \\'score\\':0.06502299010753632 #     \\'sequence\\':\\'<s> Jen la komenco de bela vivo.</s>\\' #     \\'token\\':1099 # } # { #     \\'score\\':0.0421181358397007 #     \\'sequence\\':\\'<s> Jen la komenco de bela vespero.</s>\\' #     \\'token\\':5100 # } # { #     \\'score\\':0.024884626269340515 #     \\'sequence\\':\\'<s> Jen la komenco de bela laboro.</s>\\' #     \\'token\\':1570 # } # { #     \\'score\\':0.02324388362467289 #     \\'sequence\\':\\'<s> Jen la komenco de bela tago.</s>\\' #     \\'token\\':1688 # } # { #     \\'score\\':0.020378097891807556 #     \\'sequence\\':\\'<s> Jen la komenco de bela festo.</s>\\' #     \\'token\\':4580 # } ```  > ‚Äú**Jen la komenco de bela tago**‚Äù, indeed!  With more complex prompts, you can probe whether your language model captured more semantic knowledge or even some sort of (statistical) common sense reasoning.   ## 5. Fine-tune your LM on a downstream task  We now can fine-tune our new Esperanto language model on a downstream task of **Part-of-speech tagging.**  As mentioned before, Esperanto is a highly regular language where word endings typically condition the grammatical part of speech. Using a dataset of annotated  Esperanto POS tags formatted in the CoNLL-2003 format (see example below), we can use the [`run_ner.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/run_ner.py) script from `transformers`.  > POS tagging is a token classification task just as NER so we can just use the exact same script.  ![conll](assets/01_how-to-train/conll-2003.png)  Again, here‚Äôs the hosted **[Tensorboard](https://tensorboard.dev/experiment/lOZn2wOWQo6ixpwtWyyDfQ/#scalars)** for this fine-tuning. We train for 3 epochs using a batch size of 64 per GPU.  Training and eval losses converge to small residual values as the task is rather easy (the language is regular) ‚Äì it‚Äôs still fun to be able to train it end-to-end üòÉ.  This time, let‚Äôs use a `TokenClassificationPipeline`:  ```python from transformers import TokenClassificationPipeline, pipeline   MODEL_PATH = \"./models/EsperBERTo-small-pos/\"  nlp = pipeline(     \"ner\",     model=MODEL_PATH,     tokenizer=MODEL_PATH, ) # or instantiate a TokenClassificationPipeline directly.  nlp(\"Mi estas viro kej estas tago varma.\")  # {\\'entity\\': \\'PRON\\', \\'score\\': 0.9979867339134216, \\'word\\': \\' Mi\\'} # {\\'entity\\': \\'VERB\\', \\'score\\': 0.9683094620704651, \\'word\\': \\' estas\\'} # {\\'entity\\': \\'VERB\\', \\'score\\': 0.9797462821006775, \\'word\\': \\' estas\\'} # {\\'entity\\': \\'NOUN\\', \\'score\\': 0.8509314060211182, \\'word\\': \\' tago\\'} # {\\'entity\\': \\'ADJ\\', \\'score\\': 0.9996201395988464, \\'word\\': \\' varma\\'} ```  **Looks like it worked! üî•**  <small>For a more challenging dataset for NER, <a href=\"https://github.com/stefan-it\">@stefan-it</a> recommended that we could train on the silver standard dataset from WikiANN</small>  ## 6. Share your model üéâ  Finally, when you have a nice model, please think about sharing it with the community:  - upload your model using the CLI: `transformers-cli upload` - write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:     - a model description,     - training params (dataset, preprocessing, hyperparameters),      - evaluation results,     - intended uses & limitations     - whatever else is helpful! ü§ì  ### **TADA!**  ‚û°Ô∏è Your model has a page on https://huggingface.co/models and everyone can load it using `AutoModel.from_pretrained(\"username/model_name\")`.  [![tb](assets/01_how-to-train/model_page.png)](https://huggingface.co/julien-c/EsperBERTo-small)   If you want to take a look at models in different languages, check https://huggingface.co/models  [![all models](https://huggingface.co/front/thumbnails/models.png)](https://huggingface.co/models)  ## Thank you!  ![](assets/01_how-to-train/EsperBERTo-thumbnail-v2.png) '}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"smol-project-blueprint/hf-blogs\")\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the documents\n",
    "\n",
    "To understand how to chunk the documents, we will first need to understand what our `content` column looks like. As we can see above, the content is formatted as a markdown file. Therefore, we know that our text is structured in paragraphs using header indicators like `#` or `##`, and that we can use these to split the text into chunks that are semantically meaningful. Let's write a function that does this. Similarly, we can extract things like images by using something like `BeautifulSoup.find_all(name=\"img\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 312/312 [00:07<00:00, 42.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'How to train a new language model from scratch using Transformers and Tokenizers',\n",
       " 'author': 'julien-c',\n",
       " 'date': 'February 14, 2020',\n",
       " 'local': 'how-to-train',\n",
       " 'tags': 'guide, nlp',\n",
       " 'URL': 'https://huggingface.co/blog/how-to-train',\n",
       " 'content': ' # How to train a new language model from scratch using Transformers and Tokenizers   <a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb\">     <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"> </a>  Over the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.  In this post we‚Äôll demo how to train a ‚Äúsmall‚Äù model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) ‚Äì that‚Äôs the same number of layers & heads as DistilBERT ‚Äì on **Esperanto**. We‚Äôll then fine-tune the model on a downstream task of part-of-speech tagging.  Esperanto is a *constructed language* with a goal of being easy to learn. We pick it for this demo for several reasons: - it is a relatively low-resource language (even though it‚Äôs spoken by ~2 million people) so this demo is less boring than training one more English model üòÅ - its grammar is highly regular (e.g. all common nouns end in -o, all adjectives in -a) so we should get interesting linguistic results even on a small dataset. - finally, the overarching goal at the foundation of the language is to bring people closer (fostering world peace and international understanding) which one could argue is aligned with the goal of the NLP community üíö  > N.B. You won‚Äôt need to understand Esperanto to understand this post, but if you do want to learn it, [Duolingo](https://www.duolingo.com/enroll/eo/en/Learn-Esperanto) has a nice course with 280k active learners.  Our model is going to be called‚Ä¶ wait for it‚Ä¶ **EsperBERTo** üòÇ  <img src=\"/blog/assets/01_how-to-train/eo.svg\" alt=\"Esperanto flag\" style=\"margin: auto; display: block; width: 260px;\">  ## 1. Find a dataset  First, let us find a corpus of text in Esperanto. Here we‚Äôll use the Esperanto portion of the [OSCAR corpus](https://traces1.inria.fr/oscar/) from INRIA. OSCAR is a huge multilingual corpus obtained by language classification and filtering of [Common Crawl](https://commoncrawl.org/) dumps of the Web.  <img src=\"/blog/assets/01_how-to-train/oscar.png\" style=\"margin: auto; display: block; width: 260px;\">  The Esperanto portion of the dataset is only 299M, so we‚Äôll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.  The final training corpus has a size of 3 GB, which is still small ‚Äì for your model, you will get better results the more data you can get to pretrain on.    ## 2. Train a tokenizer  We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let‚Äôs arbitrarily pick its size to be 52,000.  We recommend training a byte-level BPE (rather than let‚Äôs say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).  ```python #! pip install tokenizers  from pathlib import Path  from tokenizers import ByteLevelBPETokenizer  paths = [str(x) for x in Path(\"./eo_data/\").glob(\"**/*.txt\")]  # Initialize a tokenizer tokenizer = ByteLevelBPETokenizer()  # Customize training tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[     \"<s>\",     \"<pad>\",     \"</s>\",     \"<unk>\",     \"<mask>\", ])  # Save files to disk tokenizer.save_model(\".\", \"esperberto\") ```  And here‚Äôs a slightly accelerated capture of the output:  ![tokenizers](assets/01_how-to-train/tokenizers-fast.gif) <small>On our dataset, training took about ~5 minutes.</small>  üî•üî• Wow, that was fast! ‚ö°Ô∏èüî•  We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.  ```json { \\t\"<s>\": 0, \\t\"<pad>\": 1, \\t\"</s>\": 2, \\t\"<unk>\": 3, \\t\"<mask>\": 4, \\t\"!\": 5, \\t\"\\\\\"\": 6, \\t\"#\": 7, \\t\"$\": 8, \\t\"%\": 9, \\t\"&\": 10, \\t\"\\'\": 11, \\t\"(\": 12, \\t\")\": 13, \\t# ... }  # merges.txt l a ƒ† k o n ƒ† la t a ƒ† e ƒ† d ƒ† p # ... ```  What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto ‚Äì `ƒâ`, `ƒù`, `ƒ•`, `ƒµ`, `≈ù`, and `≈≠` ‚Äì are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.  Here‚Äôs how you can use it in `tokenizers`, including handling the RoBERTa special tokens ‚Äì of course, you‚Äôll also be able to use it directly from `transformers`.  ```python from tokenizers.implementations import ByteLevelBPETokenizer from tokenizers.processors import BertProcessing   tokenizer = ByteLevelBPETokenizer(     \"./models/EsperBERTo-small/vocab.json\",     \"./models/EsperBERTo-small/merges.txt\", ) tokenizer._tokenizer.post_processor = BertProcessing(     (\"</s>\", tokenizer.token_to_id(\"</s>\")),     (\"<s>\", tokenizer.token_to_id(\"<s>\")), ) tokenizer.enable_truncation(max_length=512)  print(     tokenizer.encode(\"Mi estas Julien.\") ) # Encoding(num_tokens=7, ...) # tokens: [\\'<s>\\', \\'Mi\\', \\'ƒ†estas\\', \\'ƒ†Juli\\', \\'en\\', \\'.\\', \\'</s>\\'] ```  ## 3. Train a language model from scratch  **Update:** The associated Colab notebook uses our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly, instead of through a script. Feel free to pick the approach you like best.  We will now train our language model using the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/main/examples/legacy/run_language_modeling.py) script from `transformers` (newly renamed from `run_lm_finetuning.py` as it now supports training from scratch more seamlessly). Just remember to leave `--model_name_or_path` to `None` to train from scratch vs. from an existing model or checkpoint.  > We‚Äôll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).  As the model is BERT-like, we‚Äôll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.  We just need to do two things: - implement a simple subclass of `Dataset` that loads data from our text files \\t- Depending on your use case, you might not even need to write your own subclass of Dataset, if one of the provided examples (`TextDataset` and `LineByLineTextDataset`) works ‚Äì but there are lots of custom tweaks that you might want to add based on what your corpus looks like. - Choose and experiment with different sets of hyperparameters.   Here‚Äôs a simple version of our EsperantoDataset.  ```python from torch.utils.data import Dataset  class EsperantoDataset(Dataset):     def __init__(self, evaluate: bool = False):         tokenizer = ByteLevelBPETokenizer(             \"./models/EsperBERTo-small/vocab.json\",             \"./models/EsperBERTo-small/merges.txt\",         )         tokenizer._tokenizer.post_processor = BertProcessing(             (\"</s>\", tokenizer.token_to_id(\"</s>\")),             (\"<s>\", tokenizer.token_to_id(\"<s>\")),         )         tokenizer.enable_truncation(max_length=512)         # or use the RobertaTokenizer from `transformers` directly.          self.examples = []          src_files = Path(\"./data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./data/\").glob(\"*-train.txt\")         for src_file in src_files:             print(\"üî•\", src_file)             lines = src_file.read_text(encoding=\"utf-8\").splitlines()             self.examples += [x.ids for x in tokenizer.encode_batch(lines)]      def __len__(self):         return len(self.examples)      def __getitem__(self, i):         # We‚Äôll pad at the batch level.         return torch.tensor(self.examples[i]) ```  If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step.  Here is one specific set of **hyper-parameters and arguments** we pass to the script:  ``` \\t--output_dir ./models/EsperBERTo-small-v1 \\t--model_type roberta \\t--mlm \\t--config_name ./models/EsperBERTo-small \\t--tokenizer_name ./models/EsperBERTo-small \\t--do_train \\t--do_eval \\t--learning_rate 1e-4 \\t--num_train_epochs 5 \\t--save_total_limit 2 \\t--save_steps 2000 \\t--per_gpu_train_batch_size 16 \\t--evaluate_during_training \\t--seed 42 ```  As usual, pick the largest batch size you can fit on your GPU(s).   **üî•üî•üî• Let‚Äôs start training!! üî•üî•üî•**  Here you can check our Tensorboard for [one particular set of hyper-parameters](https://tensorboard.dev/experiment/8AjtzdgPR1qG6bDIe1eKfw/#scalars):  [![tb](assets/01_how-to-train/tensorboard.png)](https://tensorboard.dev/experiment/8AjtzdgPR1qG6bDIe1eKfw/#scalars)  > Our example scripts log into the Tensorboard format by default, under `runs/`. Then to view your board just run `tensorboard dev upload --logdir runs` ‚Äì this will set up [tensorboard.dev](https://tensorboard.dev/), a Google-managed hosted version that lets you share your ML experiment with anyone.  ## 4. Check that the LM actually trained  Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.  Pipelines are simple wrappers around tokenizers and models, and the \\'fill-mask\\' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.  ```python from transformers import pipeline  fill_mask = pipeline(     \"fill-mask\",     model=\"./models/EsperBERTo-small\",     tokenizer=\"./models/EsperBERTo-small\" )  # The sun <mask>. # =>  result = fill_mask(\"La suno <mask>.\")  # {\\'score\\': 0.2526160776615143, \\'sequence\\': \\'<s> La suno brilis.</s>\\', \\'token\\': 10820} # {\\'score\\': 0.0999930202960968, \\'sequence\\': \\'<s> La suno lumis.</s>\\', \\'token\\': 23833} # {\\'score\\': 0.04382849484682083, \\'sequence\\': \\'<s> La suno brilas.</s>\\', \\'token\\': 15006} # {\\'score\\': 0.026011141017079353, \\'sequence\\': \\'<s> La suno falas.</s>\\', \\'token\\': 7392} # {\\'score\\': 0.016859788447618484, \\'sequence\\': \\'<s> La suno pasis.</s>\\', \\'token\\': 4552} ```  Ok, simple syntax/grammar works. Let‚Äôs try a slightly more interesting prompt:  ```python fill_mask(\"Jen la komenco de bela <mask>.\")  # This is the beginning of a beautiful <mask>. # =>  # { #     \\'score\\':0.06502299010753632 #     \\'sequence\\':\\'<s> Jen la komenco de bela vivo.</s>\\' #     \\'token\\':1099 # } # { #     \\'score\\':0.0421181358397007 #     \\'sequence\\':\\'<s> Jen la komenco de bela vespero.</s>\\' #     \\'token\\':5100 # } # { #     \\'score\\':0.024884626269340515 #     \\'sequence\\':\\'<s> Jen la komenco de bela laboro.</s>\\' #     \\'token\\':1570 # } # { #     \\'score\\':0.02324388362467289 #     \\'sequence\\':\\'<s> Jen la komenco de bela tago.</s>\\' #     \\'token\\':1688 # } # { #     \\'score\\':0.020378097891807556 #     \\'sequence\\':\\'<s> Jen la komenco de bela festo.</s>\\' #     \\'token\\':4580 # } ```  > ‚Äú**Jen la komenco de bela tago**‚Äù, indeed!  With more complex prompts, you can probe whether your language model captured more semantic knowledge or even some sort of (statistical) common sense reasoning.   ## 5. Fine-tune your LM on a downstream task  We now can fine-tune our new Esperanto language model on a downstream task of **Part-of-speech tagging.**  As mentioned before, Esperanto is a highly regular language where word endings typically condition the grammatical part of speech. Using a dataset of annotated  Esperanto POS tags formatted in the CoNLL-2003 format (see example below), we can use the [`run_ner.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/run_ner.py) script from `transformers`.  > POS tagging is a token classification task just as NER so we can just use the exact same script.  ![conll](assets/01_how-to-train/conll-2003.png)  Again, here‚Äôs the hosted **[Tensorboard](https://tensorboard.dev/experiment/lOZn2wOWQo6ixpwtWyyDfQ/#scalars)** for this fine-tuning. We train for 3 epochs using a batch size of 64 per GPU.  Training and eval losses converge to small residual values as the task is rather easy (the language is regular) ‚Äì it‚Äôs still fun to be able to train it end-to-end üòÉ.  This time, let‚Äôs use a `TokenClassificationPipeline`:  ```python from transformers import TokenClassificationPipeline, pipeline   MODEL_PATH = \"./models/EsperBERTo-small-pos/\"  nlp = pipeline(     \"ner\",     model=MODEL_PATH,     tokenizer=MODEL_PATH, ) # or instantiate a TokenClassificationPipeline directly.  nlp(\"Mi estas viro kej estas tago varma.\")  # {\\'entity\\': \\'PRON\\', \\'score\\': 0.9979867339134216, \\'word\\': \\' Mi\\'} # {\\'entity\\': \\'VERB\\', \\'score\\': 0.9683094620704651, \\'word\\': \\' estas\\'} # {\\'entity\\': \\'VERB\\', \\'score\\': 0.9797462821006775, \\'word\\': \\' estas\\'} # {\\'entity\\': \\'NOUN\\', \\'score\\': 0.8509314060211182, \\'word\\': \\' tago\\'} # {\\'entity\\': \\'ADJ\\', \\'score\\': 0.9996201395988464, \\'word\\': \\' varma\\'} ```  **Looks like it worked! üî•**  <small>For a more challenging dataset for NER, <a href=\"https://github.com/stefan-it\">@stefan-it</a> recommended that we could train on the silver standard dataset from WikiANN</small>  ## 6. Share your model üéâ  Finally, when you have a nice model, please think about sharing it with the community:  - upload your model using the CLI: `transformers-cli upload` - write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:     - a model description,     - training params (dataset, preprocessing, hyperparameters),      - evaluation results,     - intended uses & limitations     - whatever else is helpful! ü§ì  ### **TADA!**  ‚û°Ô∏è Your model has a page on https://huggingface.co/models and everyone can load it using `AutoModel.from_pretrained(\"username/model_name\")`.  [![tb](assets/01_how-to-train/model_page.png)](https://huggingface.co/julien-c/EsperBERTo-small)   If you want to take a look at models in different languages, check https://huggingface.co/models  [![all models](https://huggingface.co/front/thumbnails/models.png)](https://huggingface.co/models)  ## Thank you!  ![](assets/01_how-to-train/EsperBERTo-thumbnail-v2.png) ',\n",
       " 'chunked_content': ['5. Fine-tune your LM on a downstream task  We now can fine-tune our new Esperanto language model on a downstream task of Part-of-speech tagging.  As mentioned before, Esperanto is a highly regular language where word endings typically condition the grammatical part of speech. Using a dataset of annotated  Esperanto POS tags formatted in the CoNLL-2003 format (see example below), we can use the run_ner.py script from transformers.  > POS tagging is a token classification task just as NER so we can just use the exact same script.    Again, here‚Äôs the hosted Tensorboard for this fine-tuning. We train for 3 epochs using a batch size of 64 per GPU.  Training and eval losses converge to small residual values as the task is rather easy (the language is regular) ‚Äì it‚Äôs still fun to be able to train it end-to-end üòÉ.  This time, let‚Äôs use a TokenClassificationPipeline:  python from transformers import TokenClassificationPipeline, pipeline   MODEL_PATH = \"./models/EsperBERTo-small-pos/\"  nlp = pipeline(     \"ner\",     model=MODEL_PATH,     tokenizer=MODEL_PATH, ) # or instantiate a TokenClassificationPipeline directly.  nlp(\"Mi estas viro kej estas tago varma.\")  # {\\'entity\\': \\'PRON\\', \\'score\\': 0.9979867339134216, \\'word\\': \\' Mi\\'} # {\\'entity\\': \\'VERB\\', \\'score\\': 0.9683094620704651, \\'word\\': \\' estas\\'} # {\\'entity\\': \\'VERB\\', \\'score\\': 0.9797462821006775, \\'word\\': \\' estas\\'} # {\\'entity\\': \\'NOUN\\', \\'score\\': 0.8509314060211182, \\'word\\': \\' tago\\'} # {\\'entity\\': \\'ADJ\\', \\'score\\': 0.9996201395988464, \\'word\\': \\' varma\\'} Looks like it worked! üî• For a more challenging dataset for NER, @stefan-it recommended that we could train on the silver standard dataset from WikiANN  ',\n",
       "  '# How to train a new language model from scratch using Transformers and Tokenizers       Over the past few months, we made several improvements to our transformers and tokenizers libraries, with the goal of making it easier than ever to train a new language model from scratch.  In this post we‚Äôll demo how to train a ‚Äúsmall‚Äù model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) ‚Äì that‚Äôs the same number of layers & heads as DistilBERT ‚Äì on Esperanto. We‚Äôll then fine-tune the model on a downstream task of part-of-speech tagging.  Esperanto is a constructed language with a goal of being easy to learn. We pick it for this demo for several reasons: - it is a relatively low-resource language (even though it‚Äôs spoken by ~2 million people) so this demo is less boring than training one more English model üòÅ - its grammar is highly regular (e.g. all common nouns end in -o, all adjectives in -a) so we should get interesting linguistic results even on a small dataset. - finally, the overarching goal at the foundation of the language is to bring people closer (fostering world peace and international understanding) which one could argue is aligned with the goal of the NLP community üíö  > N.B. You won‚Äôt need to understand Esperanto to understand this post, but if you do want to learn it, Duolingo has a nice course with 280k active learners.  Our model is going to be called‚Ä¶ wait for it‚Ä¶ EsperBERTo üòÇ    ',\n",
       "  'TADA!  ‚û°Ô∏è Your model has a page on https://huggingface.co/models and everyone can load it using AutoModel.from_pretrained(\"username/model_name\").     If you want to take a look at models in different languages, check https://huggingface.co/models    ',\n",
       "  '3. Train a language model from scratch  Update: The associated Colab notebook uses our new Trainer directly, instead of through a script. Feel free to pick the approach you like best.  We will now train our language model using the run_language_modeling.py script from transformers (newly renamed from run_lm_finetuning.py as it now supports training from scratch more seamlessly). Just remember to leave --model_name_or_path to None to train from scratch vs. from an existing model or checkpoint.  > We‚Äôll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the documentation for more details).  As the model is BERT-like, we‚Äôll train it on a task of Masked language modeling, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.  We just need to do two things: - implement a simple subclass of Dataset that loads data from our text files     - Depending on your use case, you might not even need to write your own subclass of Dataset, if one of the provided examples (TextDataset and LineByLineTextDataset) works ‚Äì but there are lots of custom tweaks that you might want to add based on what your corpus looks like. - Choose and experiment with different sets of hyperparameters.   Here‚Äôs a simple version of our EsperantoDataset.  python from torch.utils.data import Dataset  class EsperantoDataset(Dataset):     def __init__(self, evaluate: bool = False):         tokenizer = ByteLevelBPETokenizer(             \"./models/EsperBERTo-small/vocab.json\",             \"./models/EsperBERTo-small/merges.txt\",         )         tokenizer._tokenizer.post_processor = BertProcessing(             (\"</s>\", tokenizer.token_to_id(\"</s>\")),             (\"<s>\", tokenizer.token_to_id(\"<s>\")),         )         tokenizer.enable_truncation(max_length=512)         # or use the RobertaTokenizer from `transformers` directly.          self.examples = []          src_files = Path(\"./data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./data/\").glob(\"*-train.txt\")         for src_file in src_files:             print(\"üî•\", src_file)             lines = src_file.read_text(encoding=\"utf-8\").splitlines()             self.examples += [x.ids for x in tokenizer.encode_batch(lines)]      def __len__(self):         return len(self.examples)      def __getitem__(self, i):         # We‚Äôll pad at the batch level.         return torch.tensor(self.examples[i])  If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step.  Here is one specific set of hyper-parameters and arguments we pass to the script:  --output_dir ./models/EsperBERTo-small-v1   --model_type roberta    --mlm   --config_name ./models/EsperBERTo-small     --tokenizer_name ./models/EsperBERTo-small  --do_train  --do_eval   --learning_rate 1e-4    --num_train_epochs 5    --save_total_limit 2    --save_steps 2000   --per_gpu_train_batch_size 16   --evaluate_during_training  --seed 42  As usual, pick the largest batch size you can fit on your GPU(s).   üî•üî•üî• Let‚Äôs start training!! üî•üî•üî•  Here you can check our Tensorboard for one particular set of hyper-parameters:    > Our example scripts log into the Tensorboard format by default, under runs/. Then to view your board just run tensorboard dev upload --logdir runs ‚Äì this will set up tensorboard.dev, a Google-managed hosted version that lets you share your ML experiment with anyone.  ',\n",
       "  'Thank you!   ',\n",
       "  '6. Share your model üéâ  Finally, when you have a nice model, please think about sharing it with the community:  - upload your model using the CLI: transformers-cli upload - write a README.md model card and add it to the repository under model_cards/. Your model card should ideally include:     - a model description,     - training params (dataset, preprocessing, hyperparameters),      - evaluation results,     - intended uses & limitations     - whatever else is helpful! ü§ì  ',\n",
       "  '2. Train a tokenizer  We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let‚Äôs arbitrarily pick its size to be 52,000.  We recommend training a byte-level BPE (rather than let‚Äôs say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more <unk> tokens!).  python #! pip install tokenizers  from pathlib import Path  from tokenizers import ByteLevelBPETokenizer  paths = [str(x) for x in Path(\"./eo_data/\").glob(\"**/*.txt\")]  # Initialize a tokenizer tokenizer = ByteLevelBPETokenizer()  # Customize training tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[     \"<s>\",     \"<pad>\",     \"</s>\",     \"<unk>\",     \"<mask>\", ])  # Save files to disk tokenizer.save_model(\".\", \"esperberto\")  And here‚Äôs a slightly accelerated capture of the output:   On our dataset, training took about ~5 minutes.  üî•üî• Wow, that was fast! ‚ö°Ô∏èüî•  We now have both a vocab.json, which is a list of the most frequent tokens ranked by frequency, and a merges.txt list of merges.  json {  \"<s>\": 0,   \"<pad>\": 1,     \"</s>\": 2,  \"<unk>\": 3,     \"<mask>\": 4,    \"!\": 5,     \"\\\\\"\": 6,    \"#\": 7,     \"$\": 8,     \"%\": 9,     \"&\": 10,    \"\\'\": 11,    \"(\": 12,    \")\": 13,    # ... }  # merges.txt l a ƒ† k o n ƒ† la t a ƒ† e ƒ† d ƒ† p # ...  What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto ‚Äì ƒâ, ƒù, ƒ•, ƒµ, ≈ù, and ≈≠ ‚Äì are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.  Here‚Äôs how you can use it in tokenizers, including handling the RoBERTa special tokens ‚Äì of course, you‚Äôll also be able to use it directly from transformers.  python from tokenizers.implementations import ByteLevelBPETokenizer from tokenizers.processors import BertProcessing   tokenizer = ByteLevelBPETokenizer(     \"./models/EsperBERTo-small/vocab.json\",     \"./models/EsperBERTo-small/merges.txt\", ) tokenizer._tokenizer.post_processor = BertProcessing(     (\"</s>\", tokenizer.token_to_id(\"</s>\")),     (\"<s>\", tokenizer.token_to_id(\"<s>\")), ) tokenizer.enable_truncation(max_length=512)  print(     tokenizer.encode(\"Mi estas Julien.\") ) # Encoding(num_tokens=7, ...) # tokens: [\\'<s>\\', \\'Mi\\', \\'ƒ†estas\\', \\'ƒ†Juli\\', \\'en\\', \\'.\\', \\'</s>\\']  ',\n",
       "  '1. Find a dataset  First, let us find a corpus of text in Esperanto. Here we‚Äôll use the Esperanto portion of the OSCAR corpus from INRIA. OSCAR is a huge multilingual corpus obtained by language classification and filtering of Common Crawl dumps of the Web.    The Esperanto portion of the dataset is only 299M, so we‚Äôll concatenate with the Esperanto sub-corpus of the Leipzig Corpora Collection, which is comprised of text from diverse sources like news, literature, and wikipedia.  The final training corpus has a size of 3 GB, which is still small ‚Äì for your model, you will get better results the more data you can get to pretrain on.    ',\n",
       "  '4. Check that the LM actually trained  Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the FillMaskPipeline.  Pipelines are simple wrappers around tokenizers and models, and the \\'fill-mask\\' one will let you input a sequence containing a masked token (here, <mask>) and return a list of the most probable filled sequences, with their probabilities.  python from transformers import pipeline  fill_mask = pipeline(     \"fill-mask\",     model=\"./models/EsperBERTo-small\",     tokenizer=\"./models/EsperBERTo-small\" )  # The sun <mask>. # =>  result = fill_mask(\"La suno <mask>.\")  # {\\'score\\': 0.2526160776615143, \\'sequence\\': \\'<s> La suno brilis.</s>\\', \\'token\\': 10820} # {\\'score\\': 0.0999930202960968, \\'sequence\\': \\'<s> La suno lumis.</s>\\', \\'token\\': 23833} # {\\'score\\': 0.04382849484682083, \\'sequence\\': \\'<s> La suno brilas.</s>\\', \\'token\\': 15006} # {\\'score\\': 0.026011141017079353, \\'sequence\\': \\'<s> La suno falas.</s>\\', \\'token\\': 7392} # {\\'score\\': 0.016859788447618484, \\'sequence\\': \\'<s> La suno pasis.</s>\\', \\'token\\': 4552}  Ok, simple syntax/grammar works. Let‚Äôs try a slightly more interesting prompt:  python fill_mask(\"Jen la komenco de bela <mask>.\")  # This is the beginning of a beautiful <mask>. # =>  # { #     \\'score\\':0.06502299010753632 #     \\'sequence\\':\\'<s> Jen la komenco de bela vivo.</s>\\' #     \\'token\\':1099 # } # { #     \\'score\\':0.0421181358397007 #     \\'sequence\\':\\'<s> Jen la komenco de bela vespero.</s>\\' #     \\'token\\':5100 # } # { #     \\'score\\':0.024884626269340515 #     \\'sequence\\':\\'<s> Jen la komenco de bela laboro.</s>\\' #     \\'token\\':1570 # } # { #     \\'score\\':0.02324388362467289 #     \\'sequence\\':\\'<s> Jen la komenco de bela tago.</s>\\' #     \\'token\\':1688 # } # { #     \\'score\\':0.020378097891807556 #     \\'sequence\\':\\'<s> Jen la komenco de bela festo.</s>\\' #     \\'token\\':4580 # }  > ‚ÄúJen la komenco de bela tago‚Äù, indeed!  With more complex prompts, you can probe whether your language model captured more semantic knowledge or even some sort of (statistical) common sense reasoning.   ']}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "counter = 0\n",
    "def structure_content(row):\n",
    "    soup = BeautifulSoup(markdown.markdown(row[\"content\"]))\n",
    "    # Split on 2 or more # followed by space\n",
    "    chunks = re.split(r'#{2,}\\s', string=soup.text)\n",
    "    # Filter empty chunks and add back the # prefix except for first chunk\n",
    "    row[\"chunked_content\"] = list(set([chunk for chunk in chunks if chunk.strip()]))\n",
    "    return row\n",
    "\n",
    "chunked_dataset = dataset.map(structure_content)\n",
    "chunked_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunked content seems reasonable, so we can now continue to the next step, which is creating embeddings for each of our content items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embeddings\n",
    "\n",
    "In order to create a vector search index, we will need to create embeddings for each of our chunks. We will use the [Hugging Face `sentence-transformers` library](https://huggingface.co/sentence-transformers) to create these embeddings. \n",
    "\n",
    "### Creating text embeddings\n",
    "\n",
    "You can find the best models using the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) but don't forget to always check the performance of the model on your specific task. We will use the [Snowflake/snowflake-arctic-embed-m-v1.5](https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v1.5) model to create the embeddings for our text, which we chose because it performs well on assymetric search on benchmarks, i.e., query-answer pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'author', 'date', 'local', 'tags', 'URL', 'content', 'chunked_content'],\n",
       "        num_rows: 312\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2927/2927 [00:41<00:00, 70.29 examples/s]\n",
      "Creating parquet from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 30.48ba/s]\n",
      "Uploading the dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/smol-blueprint-project/hf-blogs-text-embeddings/commit/b58425de991d0a84daca707495cbef7f19acaf52', commit_message='Upload dataset', commit_description='', oid='b58425de991d0a84daca707495cbef7f19acaf52', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/smol-blueprint-project/hf-blogs-text-embeddings', endpoint='https://huggingface.co', repo_type='dataset', repo_id='smol-blueprint-project/hf-blogs-text-embeddings'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "\n",
    "model = SentenceTransformer(\"Snowflake/snowflake-arctic-embed-m-v1.5\")\n",
    "\n",
    "def extract_chunks(dataset):\n",
    "    \"\"\"Extract chunks from dataset while removing unnecessary fields.\"\"\"\n",
    "    data = []\n",
    "    excluded_fields = {\"chunked_content\", \"images\", \"content\", \"code\", \"image\"}\n",
    "    \n",
    "    for row in dataset[\"train\"]:\n",
    "        for chunk in row[\"chunked_content\"]:\n",
    "            # Create new dict with only desired fields rather than copying\n",
    "            item = {}\n",
    "            for k,v in row.items():\n",
    "                if k not in excluded_fields:\n",
    "                    item[k] = v\n",
    "            item[\"chunk\"] = chunk\n",
    "            data.append(item)\n",
    "    return data\n",
    "\n",
    "def create_text_embeddings(batch):\n",
    "    \"\"\"Create embeddings for a batch of text chunks.\"\"\"\n",
    "    batch[\"embedding\"] = model.encode(batch[\"chunk\"])\n",
    "    return batch\n",
    "\n",
    "# Create dataset with chunks and generate embeddings\n",
    "chunks = extract_chunks(chunked_dataset)\n",
    "embeddings_dataset = Dataset.from_list(chunks)\n",
    "embeddings_dataset = embeddings_dataset.map(create_text_embeddings, batched=True)\n",
    "embeddings_dataset.push_to_hub(\"smol-blueprint/hf-blogs-text-embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multi-modal embeddings\n",
    "\n",
    "We can use use a similar approach to create embeddings for our images and texts. We will use the [sentence-transformers/clip-ViT-B-32](https://huggingface.co/sentence-transformers/clip-ViT-B-32) model to create the embeddings for our images and texts which will then be embedded into a single vector space. This is a larger model which means it will take longer to embed the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector search Hub datasets\n",
    "\n",
    "For the similarity search, we will can simply execute queries on top of the Hugging Face Hub using the [DuckDB integration for vector search](https://huggingface.co/docs/hub/en/datasets-duckdb). Note that we need to use the same model for embedding the query as we used for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>local</th>\n",
       "      <th>tags</th>\n",
       "      <th>URL</th>\n",
       "      <th>chunk</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Federated Learning using Hugging Face and Flower</td>\n",
       "      <td>charlesbvll</td>\n",
       "      <td>March 27, 2023</td>\n",
       "      <td>fl-with-flower</td>\n",
       "      <td>nlp, transformers, guide, flower, federated-le...</td>\n",
       "      <td>https://huggingface.co/blog/fl-with-flower</td>\n",
       "      <td>Standard Hugging Face workflow</td>\n",
       "      <td>0.109720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hugging Face Hub for Galleries, Libraries,...</td>\n",
       "      <td>davanstrien</td>\n",
       "      <td>June 12, 2023</td>\n",
       "      <td>hf-hub-glam-guide</td>\n",
       "      <td>community, guide</td>\n",
       "      <td>https://huggingface.co/blog/hf-hub-glam-guide</td>\n",
       "      <td>What can you find on the Hugging Face Hub?</td>\n",
       "      <td>0.122290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Hugging Face Accelerated Development of Wi...</td>\n",
       "      <td>Violette</td>\n",
       "      <td>March 1, 2023</td>\n",
       "      <td>classification-use-cases</td>\n",
       "      <td>nlp, case-studies</td>\n",
       "      <td>https://huggingface.co/blog/classification-use...</td>\n",
       "      <td>Solutions provided by the Hugging Face Experts...</td>\n",
       "      <td>0.132556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hugging Face's TensorFlow Philosophy</td>\n",
       "      <td>rocketknight1</td>\n",
       "      <td>August 12, 2022</td>\n",
       "      <td>tensorflow-philosophy</td>\n",
       "      <td>nlp, cv, guide</td>\n",
       "      <td>https://huggingface.co/blog/tensorflow-philosophy</td>\n",
       "      <td># Hugging Face's TensorFlow Philosophy</td>\n",
       "      <td>0.141068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Creating open machine learning datasets? Share...</td>\n",
       "      <td>davanstrien</td>\n",
       "      <td>October 30, 2023</td>\n",
       "      <td>researcher-dataset-sharing</td>\n",
       "      <td>community, research, datasets, guide</td>\n",
       "      <td>https://huggingface.co/blog/researcher-dataset...</td>\n",
       "      <td># Creating open machine learning datasets? Sha...</td>\n",
       "      <td>0.155733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title         author  \\\n",
       "0   Federated Learning using Hugging Face and Flower    charlesbvll   \n",
       "1  The Hugging Face Hub for Galleries, Libraries,...    davanstrien   \n",
       "2  How Hugging Face Accelerated Development of Wi...       Violette   \n",
       "3               Hugging Face's TensorFlow Philosophy  rocketknight1   \n",
       "4  Creating open machine learning datasets? Share...    davanstrien   \n",
       "\n",
       "               date                       local  \\\n",
       "0    March 27, 2023              fl-with-flower   \n",
       "1     June 12, 2023           hf-hub-glam-guide   \n",
       "2     March 1, 2023    classification-use-cases   \n",
       "3   August 12, 2022       tensorflow-philosophy   \n",
       "4  October 30, 2023  researcher-dataset-sharing   \n",
       "\n",
       "                                                tags  \\\n",
       "0  nlp, transformers, guide, flower, federated-le...   \n",
       "1                                   community, guide   \n",
       "2                                  nlp, case-studies   \n",
       "3                                     nlp, cv, guide   \n",
       "4               community, research, datasets, guide   \n",
       "\n",
       "                                                 URL  \\\n",
       "0         https://huggingface.co/blog/fl-with-flower   \n",
       "1      https://huggingface.co/blog/hf-hub-glam-guide   \n",
       "2  https://huggingface.co/blog/classification-use...   \n",
       "3  https://huggingface.co/blog/tensorflow-philosophy   \n",
       "4  https://huggingface.co/blog/researcher-dataset...   \n",
       "\n",
       "                                               chunk  distance  \n",
       "0                   Standard Hugging Face workflow    0.109720  \n",
       "1       What can you find on the Hugging Face Hub?    0.122290  \n",
       "2  Solutions provided by the Hugging Face Experts...  0.132556  \n",
       "3         # Hugging Face's TensorFlow Philosophy      0.141068  \n",
       "4  # Creating open machine learning datasets? Sha...  0.155733  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import duckdb\n",
    "from huggingface_hub import get_token\n",
    "\n",
    "model = SentenceTransformer(\"Snowflake/snowflake-arctic-embed-m-v1.5\")\n",
    "\n",
    "def similarity_search(\n",
    "    query: str, \n",
    "    k: int = 5, \n",
    "    dataset_name: str = \"smol-blueprint/hf-blogs-text-embeddings\", \n",
    "    embedding_column: str = \"embedding\",\n",
    "):\n",
    "    # Use same model as used for indexing\n",
    "    query_vector = model.encode(query)\n",
    "    embedding_dim = model.get_sentence_embedding_dimension()\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            title,\n",
    "            author,\n",
    "            date,\n",
    "            local,\n",
    "            tags,\n",
    "            URL,\n",
    "            chunk,\n",
    "            array_cosine_distance(\n",
    "                {embedding_column}::float[{embedding_dim}], \n",
    "                {query_vector.tolist()}::float[{embedding_dim}]\n",
    "            ) as distance\n",
    "        FROM 'hf://datasets/{dataset_name}/**/*.parquet'\n",
    "        ORDER BY distance\n",
    "        LIMIT {k}\n",
    "    \"\"\"\n",
    "    \n",
    "    return duckdb.sql(sql).to_df()\n",
    "\n",
    "similarity_search(\"What is the best way to learn Hugging Face?\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio as vector search interface\n",
    "\n",
    "We will be using Gradio as web application tool to create a demo interface for our vector search index. We can develop this locally and then easily deploy it to Hugging Face Spaces. Lastly, we can use the Gradio client as SDK to directly interact with our vector search index.\n",
    "\n",
    "### Gradio demo UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def search(query, k):\n",
    "    return similarity_search(query, k)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    query = gr.Textbox(label=\"Query\")\n",
    "    k = gr.Slider(1, 10, value=5, label=\"Number of results\")\n",
    "    btn = gr.Button(\"Search\")\n",
    "    results = gr.Dataframe(headers=[\"title\", \"url\", \"content\", \"distance\"])\n",
    "    btn.click(fn=search, inputs=[query, k], outputs=[results])\n",
    "    \n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Gradio to Hugging Face Spaces\n",
    "\n",
    "We can now [deploy our Gradio application to Hugging Face Spaces](https://huggingface.co/new-space?sdk=gradio&name=vector-search-hub). Follow the redirect and then click on the \"Create Space\" button. After that, you can copy the code from the Gradio interface and paste it into an `app.py` file. Don't forget to copy the `similarity_search` function from the notebook and paste it into the `app.py` file. Lastly, you need to create an `requirements.txt` file with and with the following content:\n",
    "\n",
    "```bash\n",
    "duckdb\n",
    "sentence-transformers\n",
    "```\n",
    "\n",
    "We wait a couple of minutes for the application to deploy et voila, we have [a public vector search interface](https://huggingface.co/spaces/smol-blueprint/vector-search-hub)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio as Rest API\n",
    "\n",
    "We can now use the [Gradio client as SDK](https://www.gradio.app/guides/getting-started-with-the-python-client) to directly interact with our vector search index. Each Gradio app has a API documentation that describes the available endpoints and their parameters, which you can access from the button at the bottom of the Gradio app's space page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidberenstein/Documents/programming/smol-project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://smol-blueprint-vector-search-hub.hf.space/ ‚úî\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>local</th>\n",
       "      <th>tags</th>\n",
       "      <th>URL</th>\n",
       "      <th>chunk</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introducing the Private Hub: A New Way to Buil...</td>\n",
       "      <td>FedericoPascual</td>\n",
       "      <td>August 3, 2022</td>\n",
       "      <td>introducing-private-hub</td>\n",
       "      <td>announcement, enterprise, hub</td>\n",
       "      <td>https://huggingface.co/blog/introducing-privat...</td>\n",
       "      <td>Training accurate models faster</td>\n",
       "      <td>0.192108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fine-tuning Llama 2 70B using PyTorch FSDP</td>\n",
       "      <td>smangrul</td>\n",
       "      <td>September 13, 2023</td>\n",
       "      <td>ram-efficient-pytorch-fsdp</td>\n",
       "      <td>llm, guide, nlp</td>\n",
       "      <td>https://huggingface.co/blog/ram-efficient-pyto...</td>\n",
       "      <td>Fine-Tuning</td>\n",
       "      <td>0.193254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Making ML-powered web games with Transformers.js</td>\n",
       "      <td>Xenova</td>\n",
       "      <td>July 5, 2023</td>\n",
       "      <td>ml-web-games</td>\n",
       "      <td>game-dev, guide, web, javascript, transformers.js</td>\n",
       "      <td>https://huggingface.co/blog/ml-web-games</td>\n",
       "      <td>1. Training the neural network</td>\n",
       "      <td>0.196486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open-Source Text Generation &amp; LLM Ecosystem at...</td>\n",
       "      <td>merve</td>\n",
       "      <td>July 17, 2023</td>\n",
       "      <td>os-llms</td>\n",
       "      <td>LLM, inference, nlp</td>\n",
       "      <td>https://huggingface.co/blog/os-llms</td>\n",
       "      <td>Tools in the Hugging Face Ecosystem for LLM Se...</td>\n",
       "      <td>0.197265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comparing the Performance of LLMs: A Deep Dive...</td>\n",
       "      <td>mehdiiraqui</td>\n",
       "      <td>November 7, 2023</td>\n",
       "      <td>Lora-for-sequence-classification-with-Roberta-...</td>\n",
       "      <td>nlp, guide, llm, peft</td>\n",
       "      <td>https://huggingface.co/blog/Lora-for-sequence-...</td>\n",
       "      <td>Pre-trained Models</td>\n",
       "      <td>0.198704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           author  \\\n",
       "0  Introducing the Private Hub: A New Way to Buil...  FedericoPascual   \n",
       "1         Fine-tuning Llama 2 70B using PyTorch FSDP         smangrul   \n",
       "2   Making ML-powered web games with Transformers.js           Xenova   \n",
       "3  Open-Source Text Generation & LLM Ecosystem at...            merve   \n",
       "4  Comparing the Performance of LLMs: A Deep Dive...      mehdiiraqui   \n",
       "\n",
       "                 date                                              local  \\\n",
       "0      August 3, 2022                            introducing-private-hub   \n",
       "1  September 13, 2023                         ram-efficient-pytorch-fsdp   \n",
       "2        July 5, 2023                                       ml-web-games   \n",
       "3       July 17, 2023                                            os-llms   \n",
       "4    November 7, 2023  Lora-for-sequence-classification-with-Roberta-...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                      announcement, enterprise, hub   \n",
       "1                                    llm, guide, nlp   \n",
       "2  game-dev, guide, web, javascript, transformers.js   \n",
       "3                                LLM, inference, nlp   \n",
       "4                              nlp, guide, llm, peft   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://huggingface.co/blog/introducing-privat...   \n",
       "1  https://huggingface.co/blog/ram-efficient-pyto...   \n",
       "2           https://huggingface.co/blog/ml-web-games   \n",
       "3                https://huggingface.co/blog/os-llms   \n",
       "4  https://huggingface.co/blog/Lora-for-sequence-...   \n",
       "\n",
       "                                               chunk  distance  \n",
       "0                  Training accurate models faster    0.192108  \n",
       "1                                      Fine-Tuning    0.193254  \n",
       "2                   1. Training the neural network    0.196486  \n",
       "3  Tools in the Hugging Face Ecosystem for LLM Se...  0.197265  \n",
       "4                               Pre-trained Models    0.198704  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "import pandas as pd\n",
    "\n",
    "client = Client(\"https://smol-blueprint-vector-search-hub.hf.space/\")\n",
    "results = client.predict(\n",
    "    api_name=\"/similarity_search\",\n",
    "    query=\"Optimizing LLM inference\", \n",
    "    k=5\n",
    ")\n",
    "pd.DataFrame(data=results[\"data\"], columns=results[\"headers\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
