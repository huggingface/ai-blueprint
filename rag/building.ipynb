{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG pipeline with a SmolLM and some rerankers\n",
    "\n",
    "We will be using the output of the [indexing](./indexing.ipynb) notebook to build a RAG pipeline. We have seen how to index the Hugging Face Hub and perform vector search on it. Now, we will build a RAG pipeline that uses this vector search index to retrieve relevant information from a company's documents and uses a SmolLM model to answer questions. Also, we will show how to use rerankers to improve the quality of the RAG pipeline.\n",
    "\n",
    "## Hugging Face as a vector search backend\n",
    "\n",
    "A brief recap of the previous notebook, we use Hugging Face as vector search backend and can call it as a REST API through the Gradio Python Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://smol-blueprint-vector-search-hub.hf.space/ âœ”\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>local</th>\n",
       "      <th>tags</th>\n",
       "      <th>URL</th>\n",
       "      <th>chunk</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introducing the Private Hub: A New Way to Buil...</td>\n",
       "      <td>FedericoPascual</td>\n",
       "      <td>August 3, 2022</td>\n",
       "      <td>introducing-private-hub</td>\n",
       "      <td>announcement, enterprise, hub</td>\n",
       "      <td>https://huggingface.co/blog/introducing-privat...</td>\n",
       "      <td>Training accurate models faster</td>\n",
       "      <td>0.192108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fine-tuning Llama 2 70B using PyTorch FSDP</td>\n",
       "      <td>smangrul</td>\n",
       "      <td>September 13, 2023</td>\n",
       "      <td>ram-efficient-pytorch-fsdp</td>\n",
       "      <td>llm, guide, nlp</td>\n",
       "      <td>https://huggingface.co/blog/ram-efficient-pyto...</td>\n",
       "      <td>Fine-Tuning</td>\n",
       "      <td>0.193254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Making ML-powered web games with Transformers.js</td>\n",
       "      <td>Xenova</td>\n",
       "      <td>July 5, 2023</td>\n",
       "      <td>ml-web-games</td>\n",
       "      <td>game-dev, guide, web, javascript, transformers.js</td>\n",
       "      <td>https://huggingface.co/blog/ml-web-games</td>\n",
       "      <td>1. Training the neural network</td>\n",
       "      <td>0.196486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open-Source Text Generation &amp; LLM Ecosystem at...</td>\n",
       "      <td>merve</td>\n",
       "      <td>July 17, 2023</td>\n",
       "      <td>os-llms</td>\n",
       "      <td>LLM, inference, nlp</td>\n",
       "      <td>https://huggingface.co/blog/os-llms</td>\n",
       "      <td>Tools in the Hugging Face Ecosystem for LLM Se...</td>\n",
       "      <td>0.197265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comparing the Performance of LLMs: A Deep Dive...</td>\n",
       "      <td>mehdiiraqui</td>\n",
       "      <td>November 7, 2023</td>\n",
       "      <td>Lora-for-sequence-classification-with-Roberta-...</td>\n",
       "      <td>nlp, guide, llm, peft</td>\n",
       "      <td>https://huggingface.co/blog/Lora-for-sequence-...</td>\n",
       "      <td>Pre-trained Models</td>\n",
       "      <td>0.198704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           author  \\\n",
       "0  Introducing the Private Hub: A New Way to Buil...  FedericoPascual   \n",
       "1         Fine-tuning Llama 2 70B using PyTorch FSDP         smangrul   \n",
       "2   Making ML-powered web games with Transformers.js           Xenova   \n",
       "3  Open-Source Text Generation & LLM Ecosystem at...            merve   \n",
       "4  Comparing the Performance of LLMs: A Deep Dive...      mehdiiraqui   \n",
       "\n",
       "                 date                                              local  \\\n",
       "0      August 3, 2022                            introducing-private-hub   \n",
       "1  September 13, 2023                         ram-efficient-pytorch-fsdp   \n",
       "2        July 5, 2023                                       ml-web-games   \n",
       "3       July 17, 2023                                            os-llms   \n",
       "4    November 7, 2023  Lora-for-sequence-classification-with-Roberta-...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                      announcement, enterprise, hub   \n",
       "1                                    llm, guide, nlp   \n",
       "2  game-dev, guide, web, javascript, transformers.js   \n",
       "3                                LLM, inference, nlp   \n",
       "4                              nlp, guide, llm, peft   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://huggingface.co/blog/introducing-privat...   \n",
       "1  https://huggingface.co/blog/ram-efficient-pyto...   \n",
       "2           https://huggingface.co/blog/ml-web-games   \n",
       "3                https://huggingface.co/blog/os-llms   \n",
       "4  https://huggingface.co/blog/Lora-for-sequence-...   \n",
       "\n",
       "                                               chunk  distance  \n",
       "0                  Training accurate models faster    0.192108  \n",
       "1                                      Fine-Tuning    0.193254  \n",
       "2                   1. Training the neural network    0.196486  \n",
       "3  Tools in the Hugging Face Ecosystem for LLM Se...  0.197265  \n",
       "4                               Pre-trained Models    0.198704  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "import pandas as pd\n",
    "\n",
    "client = Client(\"https://smol-blueprint-vector-search-hub.hf.space/\")\n",
    "\n",
    "def similarity_search(query: str, k: int = 5):\n",
    "    results = client.predict(\n",
    "        api_name=\"/similarity_search\",\n",
    "        query=query, \n",
    "        k=k\n",
    "    )\n",
    "    return pd.DataFrame(data=results[\"data\"], columns=results[\"headers\"])\n",
    "\n",
    "similarity_search(\"Optimizing LLM inference\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking retrieved documents\n",
    "\n",
    "Whenever we retrieve documents from the vector search backend, we can use a reranker to improve the quality of the retrieved documents before passing them to the LLM. We will use the [sentence-transformers library](https://huggingface.co/sentence-transformers). You can find the best models using the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard). \n",
    "\n",
    "We will first retrieve 50 documents and then use [sentence-transformers/all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) to rerank the documents and return the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>local</th>\n",
       "      <th>tags</th>\n",
       "      <th>URL</th>\n",
       "      <th>chunk</th>\n",
       "      <th>distance</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>StarCoder: A State-of-the-Art LLM for Code</td>\n",
       "      <td>lvwerra</td>\n",
       "      <td>May 4, 2023</td>\n",
       "      <td>starcoder</td>\n",
       "      <td>nlp, community, research</td>\n",
       "      <td>https://huggingface.co/blog/starcoder</td>\n",
       "      <td># StarCoder: A State-of-the-Art LLM for Code</td>\n",
       "      <td>0.240642</td>\n",
       "      <td>0.494235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Deploy Hugging Face models easily with Amazon ...</td>\n",
       "      <td>philschmid</td>\n",
       "      <td>July 8, 2021</td>\n",
       "      <td>deploy-hugging-face-models-easily-with-amazon-...</td>\n",
       "      <td>guide, partnerships, aws</td>\n",
       "      <td>https://huggingface.co/blog/deploy-hugging-fac...</td>\n",
       "      <td>pseudo code end</td>\n",
       "      <td>0.245591</td>\n",
       "      <td>0.493449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Porting fairseq wmt19 translation system to tr...</td>\n",
       "      <td>stas</td>\n",
       "      <td>November 3, 2020</td>\n",
       "      <td>porting-fsmt</td>\n",
       "      <td>open-source-collab, nlp</td>\n",
       "      <td>https://huggingface.co/blog/porting-fsmt</td>\n",
       "      <td>Notes</td>\n",
       "      <td>0.239382</td>\n",
       "      <td>0.493295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Open-Source Text Generation &amp; LLM Ecosystem at...</td>\n",
       "      <td>merve</td>\n",
       "      <td>July 17, 2023</td>\n",
       "      <td>os-llms</td>\n",
       "      <td>LLM, inference, nlp</td>\n",
       "      <td>https://huggingface.co/blog/os-llms</td>\n",
       "      <td>Tools in the Hugging Face Ecosystem for LLM Se...</td>\n",
       "      <td>0.233712</td>\n",
       "      <td>0.492880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Block Sparse Matrices for Smaller and Faster L...</td>\n",
       "      <td>madlag</td>\n",
       "      <td>Sep 10, 2020</td>\n",
       "      <td>pytorch_block_sparse</td>\n",
       "      <td>research, nlp</td>\n",
       "      <td>https://huggingface.co/blog/pytorch_block_sparse</td>\n",
       "      <td># Block Sparse Matrices for Smaller and Faster...</td>\n",
       "      <td>0.234537</td>\n",
       "      <td>0.492682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      author  \\\n",
       "8         StarCoder: A State-of-the-Art LLM for Code     lvwerra   \n",
       "9  Deploy Hugging Face models easily with Amazon ...  philschmid   \n",
       "6  Porting fairseq wmt19 translation system to tr...        stas   \n",
       "2  Open-Source Text Generation & LLM Ecosystem at...       merve   \n",
       "3  Block Sparse Matrices for Smaller and Faster L...      madlag   \n",
       "\n",
       "               date                                              local  \\\n",
       "8       May 4, 2023                                          starcoder   \n",
       "9      July 8, 2021  deploy-hugging-face-models-easily-with-amazon-...   \n",
       "6  November 3, 2020                                       porting-fsmt   \n",
       "2     July 17, 2023                                            os-llms   \n",
       "3      Sep 10, 2020                               pytorch_block_sparse   \n",
       "\n",
       "                       tags  \\\n",
       "8  nlp, community, research   \n",
       "9  guide, partnerships, aws   \n",
       "6   open-source-collab, nlp   \n",
       "2       LLM, inference, nlp   \n",
       "3             research, nlp   \n",
       "\n",
       "                                                 URL  \\\n",
       "8              https://huggingface.co/blog/starcoder   \n",
       "9  https://huggingface.co/blog/deploy-hugging-fac...   \n",
       "6           https://huggingface.co/blog/porting-fsmt   \n",
       "2                https://huggingface.co/blog/os-llms   \n",
       "3   https://huggingface.co/blog/pytorch_block_sparse   \n",
       "\n",
       "                                               chunk  distance      rank  \n",
       "8    # StarCoder: A State-of-the-Art LLM for Code     0.240642  0.494235  \n",
       "9                                   pseudo code end   0.245591  0.493449  \n",
       "6                                            Notes    0.239382  0.493295  \n",
       "2  Tools in the Hugging Face Ecosystem for LLM Se...  0.233712  0.492880  \n",
       "3  # Block Sparse Matrices for Smaller and Faster...  0.234537  0.492682  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "def rerank_retrieved_documents(query: str, k_retrieved: int = 10, k_reranked: int = 5):\n",
    "    documents = similarity_search(query, k_retrieved)\n",
    "    documents[\"rank\"] = reranker.predict([[query, hit] for hit in documents[\"chunk\"]])\n",
    "    documents = documents.sort_values(by=\"rank\", ascending=False)\n",
    "    return documents[:k_reranked]\n",
    "\n",
    "rerank_retrieved_documents(\"How can I optimize LLM inference?\", k_retrieved=10, k_reranked=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
